# -*- coding: utf-8 -*-
"""trainclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PNhAfTQLChs6L26SL_AKQYNhx_of5NSE
"""

import numpy as np
import os
import argparse
import time
import librosa
import pickle
from tqdm import tqdm
import torch
import re
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.spectral_norm as spectral_norm
import IPython.display as display
import gc
from torch.utils.data.dataset import Dataset

from google.colab import drive
drive.mount("/content/drive")

path="/content/drive/MyDrive/My_CycleGAN"
os.chdir(path)

from generatordiscriminator import Generator,Discriminator

class trainingDataset(Dataset):
    def __init__(self, datasetA, datasetB, n_frames=64):
        self.datasetA = datasetA
        self.datasetB = datasetB
        self.n_frames = n_frames

    def __getitem__(self, index):
        return self.datasetA[index], self.datasetB[index]

    def __len__(self):
        return min((self.datasetA).shape[0], self.datasetB.shape[0])

class CycleGANTraining(object):
    def __init__(self,
          cache_folder,
          model_checkpoint,
          validation_A_dir,
          output_A_dir,
          validation_B_dir,
          output_B_dir,
          restart_training_at=None):
      
        self.start_epoch = 0
        self.num_epochs = 500000//70 # 500k iterations
        # epoch——使用整个训练样本集传播一次。
        # iteration——使用batch size个样本传播一次。

        self.mini_batch_size = 1 # batch size of 1

        # 训练集
        npzfile = np.load(os.path.join(cache_folder, 'train_dataset.npz'))
        A = npzfile['A']
        B = npzfile['B']
        # 4d: [batch_size=1/70, channels=1, height=features=80, width=T=64]
        self.dataset_A = torch.tensor(A).unsqueeze(1).float()
        self.dataset_B = torch.tensor(B).unsqueeze(1).float()

        # Speech Parameters
        cache_folder = './cache/'
        stats = np.load(os.path.join(cache_folder, 'norm_stats.npz'))
        self.mean_A=stats['mean_A']
        self.std_A=stats['std_A']
        self.mean_B=stats['mean_B']
        self.std_B=stats['std_B']

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Generator and Discriminator
        self.generator_A2B = Generator().to(self.device)
        self.generator_B2A = Generator().to(self.device)
        self.discriminator_A = Discriminator().to(self.device)
        self.discriminator_B = Discriminator().to(self.device)

        # Loss Functions
        criterion_mse = torch.nn.MSELoss() # least square GAN

        # Optimizer
        g_params = list(self.generator_A2B.parameters()) + list(self.generator_B2A.parameters())
        d_params = list(self.discriminator_A.parameters()) + list(self.discriminator_B.parameters())

        # Initial learning rates
        # # The learning rates were set to 0.0002 for the generators and 0.0001 for the discriminators 
        self.generator_lr = 2e-4  # 0.0002
        self.discriminator_lr = 1e-4  # 0.0001

        # Starts learning rate decay from after this many iterations have passed
        # We kept the same learning rate for the first 2×10^5 iterations 
        self.start_decay = 200000  

        # Learning rate decay
        # linearly decay over the next 2 × 105 iterations. 
        self.generator_lr_decay = self.generator_lr / 200000
        self.discriminator_lr_decay = self.discriminator_lr / 200000

        # with momentum terms β1 and β2 of 0.5 and 0.999, respectively. 
        # using the Adam optimizer
        self.generator_optimizer = torch.optim.Adam(
            g_params, lr=self.generator_lr, betas=(0.5, 0.999))
        self.discriminator_optimizer = torch.optim.Adam(
            d_params, lr=self.discriminator_lr, betas=(0.5, 0.999))

        # To Load save previously saved models
        self.modelCheckpoint = model_checkpoint
        os.makedirs(self.modelCheckpoint, exist_ok=True)

        # Validation set Parameters
        self.validation_A_dir = validation_A_dir
        self.output_A_dir = output_A_dir
        os.makedirs(self.output_A_dir, exist_ok=True)
        self.validation_B_dir = validation_B_dir
        self.output_B_dir = output_B_dir
        os.makedirs(self.output_B_dir, exist_ok=True)
        self.vocoder = torch.hub.load('descriptinc/melgan-neurips', 'load_melgan')

        # Storing Discriminatior and Generator Loss
        self.generator_loss_store = []
        self.discriminator_loss_store = []

        self.file_name = 'log_store_non_sigmoid.txt'

        if restart_training_at is not None:
            # Training will resume from previous checkpoint
            self.start_epoch = self.loadModel(restart_training_at)
            print("Training resumed")

    def adjust_lr_rate(self, optimizer, name='generator'):
        if name == 'generator':
            self.generator_lr = max(
                0., self.generator_lr - self.generator_lr_decay)
            for param_groups in optimizer.param_groups:
                param_groups['lr'] = self.generator_lr
        else:
            self.discriminator_lr = max(
                0., self.discriminator_lr - self.discriminator_lr_decay)
            for param_groups in optimizer.param_groups:
                param_groups['lr'] = self.discriminator_lr

    def reset_grad(self):
        self.generator_optimizer.zero_grad()
        self.discriminator_optimizer.zero_grad()

    def train(self):
        # Training Begins
        for epoch in range(self.start_epoch, self.num_epochs):
            print('epoch :', epoch)
            start_time_epoch = time.time()

            # Constants
            # λcyc and λid were set to 10 and 5, respectively,
            cycle_loss_lambda = 10
            identity_loss_lambda = 5

            # Preparing Dataset
            n_samples = self.dataset_A.shape[0] # 70

            dataset = trainingDataset(datasetA=self.dataset_A,
                          datasetB=self.dataset_B,
                          n_frames=64)
            train_loader = torch.utils.data.DataLoader(dataset=dataset,
                          batch_size=self.mini_batch_size,
                          shuffle=True,
                          drop_last=False)

            pbar = tqdm(enumerate(train_loader))
            print(len(train_loader))
            for i, (real_A, real_B) in enumerate(train_loader):
                num_iterations = (n_samples // self.mini_batch_size) * epoch + i
                # print("iteration no: ", num_iterations, epoch)

                if num_iterations > 10000:    # Lid was used only for the first 10k iterations.
                    identity_loss_lambda = 0

                if num_iterations > self.start_decay:
                    self.adjust_lr_rate(
                        self.generator_optimizer, name='generator')
                    self.adjust_lr_rate(
                        self.generator_optimizer, name='discriminator')

                real_A = real_A.to(self.device).float()
                real_B = real_B.to(self.device).float()

                ####################################
                # Generator Loss function
                ####################################

                fake_B = self.generator_A2B(real_A)
                cycle_A = self.generator_B2A(fake_B)

                fake_A = self.generator_B2A(real_B)
                cycle_B = self.generator_A2B(fake_A)

                identity_A = self.generator_B2A(real_A) # F(x)
                identity_B = self.generator_A2B(real_B) # G(y)

                d_fake_A = self.discriminator_A(fake_A) 
                d_fake_B = self.discriminator_B(fake_B)

                d_real_A = self.discriminator_A(real_A) 
                d_real_B = self.discriminator_B(real_B)

                # for the second step adverserial loss # 新的两个discriminators?
                d_fake_cycle_A = self.discriminator_A(cycle_A)
                d_fake_cycle_B = self.discriminator_B(cycle_B)

                # Generator Cycle loss 2
                cycleLoss = torch.mean(
                    torch.abs(real_A - cycle_A)) + torch.mean(torch.abs(real_B - cycle_B))

                # Generator Identity Loss 3
                identiyLoss = torch.mean(
                    torch.abs(real_A - identity_A)) + torch.mean(torch.abs(real_B - identity_B))

                # Generator Adversarial Losses 1
                generator_loss_A2B = torch.mean((1 - d_fake_B) ** 2) #torch.mean(torch.log(1 - d_fake_B)) 
                generator_loss_B2A = torch.mean((1 - d_fake_A) ** 2) #torch.mean(torch.log(1 - d_fake_A)) 

                # Total Generator Loss
                generator_loss = generator_loss_A2B + generator_loss_B2A + \
                        cycle_loss_lambda * cycleLoss + identity_loss_lambda * identiyLoss
                self.generator_loss_store.append(generator_loss.item())

                # Backprop for Generator
                self.reset_grad()
                generator_loss.backward()
                self.generator_optimizer.step()

                ####################################
                # Discriminator Loss Function ???????????????????????????????
                ####################################

                # Discriminator Feed Forward
                d_real_A = self.discriminator_A(real_A)
                d_real_B = self.discriminator_B(real_B)

                generated_A = self.generator_B2A(real_B)
                d_fake_A = self.discriminator_A(generated_A)

                # for the second step adverserial loss
                cycled_B = self.generator_A2B(generated_A)
                d_cycled_B = self.discriminator_B(cycled_B)

                generated_B = self.generator_A2B(real_A)
                d_fake_B = self.discriminator_B(generated_B)

                # for the second step adverserial loss
                cycled_A = self.generator_B2A(generated_B)
                d_cycled_A = self.discriminator_A(cycled_A)

                # Loss Functions
                d_loss_A_real = torch.mean((1 - d_real_A) ** 2)
                d_loss_A_fake = torch.mean((0 - d_fake_A) ** 2)
                d_loss_A = (d_loss_A_real + d_loss_A_fake) / 2.0

                d_loss_B_real = torch.mean((1 - d_real_B) ** 2)
                d_loss_B_fake = torch.mean((0 - d_fake_B) ** 2)
                d_loss_B = (d_loss_B_real + d_loss_B_fake) / 2.0

                # the second step adverserial loss
                d_loss_A_cycled = torch.mean((0 - d_cycled_A) ** 2)
                d_loss_B_cycled = torch.mean((0 - d_cycled_B) ** 2)
                d_loss_A_2nd = (d_loss_A_real + d_loss_A_cycled) / 2.0
                d_loss_B_2nd = (d_loss_B_real + d_loss_B_cycled) / 2.0

                # Final Loss for discriminator with the second step adverserial loss
                d_loss = (d_loss_A + d_loss_B) / 2.0 + (d_loss_A_2nd + d_loss_B_2nd) / 2.0
                self.discriminator_loss_store.append(d_loss.item())

                # Backprop for Discriminator
                self.reset_grad()
                d_loss.backward()
                self.discriminator_optimizer.step()

                if (i + 1) % 10 == 0:
                    pbar.set_description(
                        "Iter:{} Generator Loss:{:.4f} Discrimator Loss:{:.4f} GA2B:{:.4f} GB2A:{:.4f} G_id:{:.4f} G_cyc:{:.4f} D_A:{:.4f} D_B:{:.4f}".format(
                            num_iterations,
                            generator_loss.item(),
                            # loss['generator_loss'],
                            d_loss.item(), generator_loss_A2B, generator_loss_B2A, identiyLoss, cycleLoss, d_loss_A,
                            d_loss_B))
                gc.collect()

            if epoch % 2 == 0 and epoch != 0:
                end_time = time.time()
                store_to_file = "Epoch: {} Generator Loss: {:.4f} Discriminator Loss: {}, Time: {:.2f}\n\n".format(
                    epoch, generator_loss.item(), d_loss.item(), end_time - start_time_epoch)
                self.store_to_file(store_to_file)
                print(store_to_file)

                # Save the Entire model
                print("Saving model Checkpoint  ......")
                store_to_file = "Saving model Checkpoint  ......"
                self.store_to_file(store_to_file)
                self.saveModelCheckPoint(epoch, '{}'.format(
                    self.modelCheckpoint + '_CycleGAN_CheckPoint'))
                print("Model Saved!")
            
            if epoch % 5 == 0 and epoch != 0:
                # Validation Set
                validation_start_time = time.time()
                self.validation_for_A_dir()
                self.validation_for_B_dir()
                validation_end_time = time.time()
                store_to_file = "Time taken for validation Set: {}".format(
                    validation_end_time - validation_start_time)
                self.store_to_file(store_to_file)
                print("Time taken for validation Set: {}".format(
                    validation_end_time - validation_start_time))
            
                
    def validation_for_A_dir(self):
        print("Generating Validation Data B from A...")
        for file in os.listdir(self.validation_A_dir):
            wavpath = os.path.join(self.validation_A_dir, file)
            wav_orig, _ = librosa.load(wavpath, sr=22050, mono=True)
            #display.display(display.Audio(wav_orig, rate=22050))
            spec = self.vocoder(torch.tensor([wav_orig]))
            mel = spec.cpu().detach().numpy()[0]

            app = (np.ma.log(mel) - self.mean_A) / self.std_A
            #print('app ', app.shape)

            coded_sp_norm = np.array([[app]]) # [:,:64]
            #print('coded_sp_norm ', coded_sp_norm.shape)

            if torch.cuda.is_available():
                coded_sp_norm = torch.from_numpy(coded_sp_norm).cuda().float()
            else:
                coded_sp_norm = torch.from_numpy(coded_sp_norm).float()

            coded_sp_converted_norm = self.generator_A2B(coded_sp_norm)
            #print('coded_sp_converted_norm 1 ', coded_sp_converted_norm.shape)

            if torch.cuda.is_available():
              coded_sp_converted_norm = coded_sp_converted_norm.cpu().detach().numpy() 
            else:
              coded_sp_converted_norm = coded_sp_converted_norm.detach().numpy() 

            #print('coded_sp_converted_norm 2 ', coded_sp_converted_norm.shape)
            t = app.shape[-1]
            coded_sp_converted_norm = np.ma.masked_array(coded_sp_converted_norm[0,0,:,:t])
            #print('coded_sp_converted_norm 3 ', coded_sp_converted_norm.shape)



            new_app = np.ma.masked_where(np.ma.getmask(app), coded_sp_converted_norm) # [:,:64] applies the mask of m on x

            denorm_converted = np.ma.exp(new_app * self.std_B + self.mean_B)
            rev = self.vocoder.inverse(torch.tensor(np.array([denorm_converted])).float()) 
            #display.display(display.Audio(rev.cpu().detach().numpy()[0], rate=22050))

            librosa.output.write_wav(path=os.path.join(self.output_A_dir, os.path.basename(file)),
                          y=rev.cpu().detach().numpy()[0],
                          sr=22050)
    def validation_for_B_dir(self):
        print("Generating Validation Data A from B...")
        for file in os.listdir(self.validation_B_dir):
            wavpath = os.path.join(self.validation_B_dir, file)
            wav_orig, _ = librosa.load(wavpath, sr=22050, mono=True)
            #display.display(display.Audio(wav_orig, rate=22050))
            spec = self.vocoder(torch.tensor([wav_orig]))
            mel = spec.cpu().detach().numpy()[0]

            app = (np.ma.log(mel) - self.mean_B) / self.std_B

            coded_sp_norm = np.array([[app]]) # [:,:64]

            if torch.cuda.is_available():
                coded_sp_norm = torch.from_numpy(coded_sp_norm).cuda().float()
            else:
                coded_sp_norm = torch.from_numpy(coded_sp_norm).float()

            coded_sp_converted_norm = self.generator_B2A(coded_sp_norm)

            if torch.cuda.is_available():
              coded_sp_converted_norm = coded_sp_converted_norm.cpu().detach().numpy() 
            else:
              coded_sp_converted_norm = coded_sp_converted_norm.detach().numpy() 

            t = app.shape[-1]
            coded_sp_converted_norm = np.ma.masked_array(coded_sp_converted_norm[0,0,:,:t])

            new_app = np.ma.masked_where(np.ma.getmask(app), coded_sp_converted_norm) # [:,:64] applies the mask of m on x

            denorm_converted = np.ma.exp(new_app * self.std_A + self.mean_A)
            rev = self.vocoder.inverse(torch.tensor(np.array([denorm_converted])).float()) 
            #display.display(display.Audio(rev.cpu().detach().numpy()[0], rate=22050))

            librosa.output.write_wav(path=os.path.join(self.output_B_dir, os.path.basename(file)),
                          y=rev.cpu().detach().numpy()[0],
                          sr=22050)

    def savePickle(self, variable, fileName):
        with open(fileName, 'wb') as f:
            pickle.dump(variable, f)

    def loadPickleFile(self, fileName):
        with open(fileName, 'rb') as f:
            return pickle.load(f)

    def store_to_file(self, doc):
        doc = doc + "\n"
        with open(self.file_name, "a") as myfile:
            myfile.write(doc)

    def saveModelCheckPoint(self, epoch, PATH):
        torch.save({
            'epoch': epoch,
            'generator_loss_store': self.generator_loss_store,
            'discriminator_loss_store': self.discriminator_loss_store,
            'model_genA2B_state_dict': self.generator_A2B.state_dict(),
            'model_genB2A_state_dict': self.generator_B2A.state_dict(),
            'model_discriminatorA': self.discriminator_A.state_dict(),
            'model_discriminatorB': self.discriminator_B.state_dict(),
            'generator_optimizer': self.generator_optimizer.state_dict(),
            'discriminator_optimizer': self.discriminator_optimizer.state_dict()
        }, PATH)

    def loadModel(self, PATH):
        checkPoint = torch.load(PATH)
        self.generator_A2B.load_state_dict(
            state_dict=checkPoint['model_genA2B_state_dict'])
        self.generator_B2A.load_state_dict(
            state_dict=checkPoint['model_genB2A_state_dict'])
        self.discriminator_A.load_state_dict(
            state_dict=checkPoint['model_discriminatorA'])
        self.discriminator_B.load_state_dict(
            state_dict=checkPoint['model_discriminatorB'])
        self.generator_optimizer.load_state_dict(
            state_dict=checkPoint['generator_optimizer'])
        self.discriminator_optimizer.load_state_dict(
            state_dict=checkPoint['discriminator_optimizer'])
        epoch = int(checkPoint['epoch']) + 1
        self.generator_loss_store = checkPoint['generator_loss_store']
        self.discriminator_loss_store = checkPoint['discriminator_loss_store']
        return epoch